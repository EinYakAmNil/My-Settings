#!/bin/env python3
from bs4 import BeautifulSoup
import subprocess
import requests
import argparse
import os

def is_img(tag):
    """Helper function for image extraction on Mangakakalot/Readmangato
    """
    return tag.has_attr('title') and tag.has_attr('src')

def readmanganato_chapters(url):
    """Returns the links to the chapters and their names of Readmangato in a
    dictionary
    """
    sauce = requests.get(url).text
    mid = url.split("/")[-1]
    soup = BeautifulSoup(sauce, "html.parser")
    chapters = [i for i in soup.find_all("a")  if
        i.has_attr("href") and
        "chapter" in i["href"] and
        mid in i["href"]
    ]
    names = [i.text for i in chapters]
    links = [i["href"] for i in chapters]

    return dict(zip(names, links))

def mangakakalot_chapters(url):
    """Returns the links to the chapters and their names of Mangakakalot in a
    dictionary
    """
    sauce = requests.get(url).text
    soup = BeautifulSoup(sauce, "html.parser")
    title = url.split("/")[-1]
    chapters = [i for i in soup.find_all("a")  if
        i.has_attr("href") and
        title in i["href"] and
        "chapter" in i["href"]
    ]
    names = [i.text for i in chapters]
    links = [i["href"] for i in chapters]

    return dict(zip(names, links))

def mangakakalot_nelo_img(url):
    """Downloads the panels of a chapter on Mangakakalot/Readmanganato
    """
    session = requests.get(url)
    sauce = session.text
    soup = BeautifulSoup(sauce, 'html.parser')

# Use bare title in html to find relevant links later on
    title = soup.title.text

# Determine the correct extractor for each website
    if "mangakakalot" in url:
        title = title.replace(" - Mangakakalot.com", "")

    if "manganelo" in url:
        title = title.replace(" - Manganelo", "")

    if "readmanganato" in url:
        title = title.replace(" - Manganelo", "")
#Find image links...
    imgs = [ i for i in soup.find_all(is_img) if title.lower() in i["alt"].lower() ]

#...and download them using curl to do referer policy correctly
    j = 1
    for i in imgs:
        img = subprocess.run(
                ["curl", "--referer", url, i["src"]],
                capture_output=True
                )
        with open(str(j)+".jpg", 'wb+') as handle:
            handle.write(img.stdout)

        j += 1

    return None

def mangapill_chapters(url):
    """Returns the links to the chapters and their Number of Mangapill in a
    dictionary
    """
    sauce = requests.get(url).text
    soup = BeautifulSoup(sauce, 'html.parser')
    a = soup.find_all("a")

    chapters = [i for i in a if i.has_attr("class") and "border" in i["class"]]
    names = [i.text for i in chapters]
    links = ["https://mangapill.com"+i["href"] for i in chapters]

    return dict(zip(names, links))

def mangapill_img(url):
    """Downloads the panels of a chapter on Mangapill
    """
    sauce = requests.get(url).text
    soup = BeautifulSoup(sauce, 'html.parser')
    img = soup.find_all("img")

    j = 1
    for i in img:
        panel = requests.get(i["data-src"]).content

        with open(str(j)+".jpg", 'wb+') as handle:
            handle.write(panel)

        j += 1

    return None

argp = argparse.ArgumentParser()
argp.add_argument(
    "URL",
    action='store',
    nargs=1,
    help="URL of the manga to download."
)
argp.add_argument(
    "-d", "--destination",
    action='store',
    nargs=1,
    help="Where the Download is saved",
    default="$HOME/Downloads"
)
args = argp.parse_args()
url = args.URL[0]
dest = os.path.expandvars(args.destination)

# Go to target location and prepare the primary folder
os.chdir(dest)
dir_name = url.replace("/", "")
os.makedirs(dir_name)
os.chdir(dir_name)

# Detect from which parsers to use
if "mangakakalot" in url:
    chapters = mangakakalot_chapters(url) 
# Create a subdirectory for each chapter
    for i, j in chapters.items():
        os.makedirs(i)
        os.chdir(i)
        print("Downloading " + i + " of " + url +"...")
        mangakakalot_nelo_img(j)
        os.chdir('..')

elif "manganato" in url:
    chapters = readmanganato_chapters(url) 
# Create a subdirectory for each chapter
    for i, j in chapters.items():
        os.makedirs(i)
        os.chdir(i)
        print("Downloading " + i + " of " + url +"...")
        mangakakalot_nelo_img(j)
        os.chdir('..')

elif "mangapill" in url:
    chapters = mangapill_chapters(url) 
# Create a subdirectory for each chapter
    for i, j in chapters.items():
        os.makedirs(i)
        os.chdir(i)
        print("Downloading " + i + " of " + url +"...")
        mangapill_img(j)
        os.chdir('..')

else:
    raise ValueError("No suitable parser")
